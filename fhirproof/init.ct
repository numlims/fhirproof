# fhirproof: fhir import checker

die fhirproof klasse instantiiert die einzelnen checks als klassen und
ruft sie jeweils fuer einen entry auf.

``//__init__.py:

``import``

# fhirproof reads fhir json from stdin and checks the entries
class fhirproof:

    ``vars``
    ``init``

    ``check``

    ``check_entries``

    ``setuplog``
    ``entries_from_dir``
    ``parent``
  
``

init uebernimmt variablen und setzt die datenbankverbindung und
traction auf.

``/init:
    # init inits fhirproof with db target, input file, centraxx user, log file and config
    def __init__(self, dbtarget, user, logfile, configpath:str=None):

        self.dbtarget = dbtarget

        # connect to db if target given
        self.db = None
        self.tr = None
        if dbtarget != None:
            self.db = dbcq(dbtarget)
            # traction for queries
            self.tr = tr.traction(self.db)

        self.user = user
        self.logfile = logfile

        self._setuplog(logfile)

        # read config file yaml
        with open(configpath, "r") as file:
             self.config = yaml.safe_load(file)
        
        # is the input ready for centraxx import?
        self.ok = True
``

importiere traction.

``/import:
import tr
from dbcq import dbcq
import yaml
``

initialisiere variablen, die die einzelnen check klassen einsehen koennen.

``/vars:
    # these variables are accessible to the fhirchecks
    entrybyfhirid = {} # entries referenced by fullUrl / fhirids, e.g. "Specimen/1037700"
    entrybysampleid = {} # entries referenced by sampleid
    shouldzerorest = {} # should restmenge be zero
    aqtgchildless = {} # is a aliquotgroup without children?
``

check both specimen and observations, depending on an entry's resourceType.

``/check:
def check(self, dir, encoding=None):

  # the collected entries
  entries = self.entries_from_dir(dir, encoding)

  self.check_entries(entries)
``

check_entries checkt die entries specimen or observation.

``/check_entries:
    def check_entries(self, entries):
        ``setup``
        ``loop``
        ``postloop``
``

setze ok zurueck und drucke eine start nachricht.

``./setup:
        self.ok = True

        self.log.info(f"starting check against {self.dbtarget}")
``

initialisiere die checks und fuehre drei counter ein fuer
aliquotgroups, masters und deriveds.

``
        # initialize checks
        aqtmat = AqtMatCheck(self)
        primary_in_db = PrimaryInDbCheck(self)
        dates = DatesCheck(self)
        location = LocationCheck(self)
        behealter = BehealterCheck(self)
        ou = OUCheck(self)
        parenting = ParentingCheck(self)
        psn = PsnCheck(self)
        restmenge = RestmengeCheck(self)
        derivmat = DerivmatCheck(self)
        mayeditou = MayUserEditOUCheck(self)
        idcontainer = IdContainerCheck(self)

        # count for some stats
        aqtg_count = 0
        master_count = 0
        derived_count = 0
``

der loop leauft ueber jeden entry.

``/check_entries/loop:
        # run checks
        for entry in entries:
            ``.``
``

speicher den entry nach seiner fhirid.

check depending on specimen or observation.

``
            # keep arrays up to date
            self.entrybyfhirid[dig(entry, "fullUrl")] = entry

            resource = dig(entry, "resource")
            if fh.resourceType(resource) == "Specimen":
              ``specimen``
            elif fh.resourceType(resource) == "Observation":
              ``observation``

``

wie kommt schnell an verschachtelte dict values ran ohne viele
none-checks? wenn man die entries als DictPath speichert, funktioniert
get ohne none-checks, allerdings gibt der getter das returnte dict
nicht als DictPath, sondern als plain dict zurueck, woraus man wieder
einen DictPath machen muesste. dig ist eine shorthand fuer
extract_dict aus dict_path.


beginne mit den aliquot checks. für aliquotgruppen gibt es keine
sampleid. weil wir für relativ viele checks die `sampleid` benutzen,
gehen wir zum nächsten check, wenn es im json keine sampleid
gibt. aliquotgruppen haben keine sampleid, mach ihre checks bevor wir
weitergehen.

``/check_entries/loop/specimen:
            if fh.type(dig(entry, 'resource')) == "ALIQUOTGROUP":
                aqtg_count += 1
                if not dig(entry, "fullUrl") in self.aqtgchildless: # tmp way to prohibit overwrites
                    self.aqtgchildless[dig(entry, "fullUrl")] = True
                aqtmat.check(entry)

            # print(f"entry resource: {json.dumps(dig(entry, 'resource'))}")
``

wenn es keine sampleid gibt, skippe zum naechsten entry.

``
            sampleid = fh.sampleid(dig(entry, 'resource'))
            if sampleid == None:
                continue
``

speichere den entry anhand der sampleid, zaehle master und derived samples.

``
            self.entrybysampleid[sampleid] = entry

            if fh.type(dig(entry, 'resource')) == "DERIVED":
                derived_count += 1
            if fh.type(dig(entry, 'resource')) == "MASTER":
                master_count += 1
``

lass die checks fuer die nicht-aliquot (derived) samples laufen. warum nicht-aliquot?

``
            # primary in db
            primary_in_db.check(entry)
            # dates
            dates.check(entry)
            # location
            location.check(entry)
            # behealter
            behealter.check(entry)
            # org
            ou.check(entry)
            # parenting
            parenting.check(entry)
            # psn
            psn.check(entry)
            # restmenge
            restmenge.check(entry)
            # derived material
            derivmat.check(entry)
            # edit oe?
            # mayeditou.check(entry, self.user)
            # id container
            idcontainer.check(entry)
``

if the resource is an observation, do the observation checks.

``/check_entries/loop/observation:
            print("todo check observation")
``

restmenge und parenting machen einen extra check nach dem loop.

``/check_entries/postloop:
        restmenge.end()
        parenting.end()
``

zeige an, wie viele aliquot groups / masters / derived gecheckt
wurden. gib zurueck, ob es einen fehler gab oder nicht. self.ok wird
beschrieben von der err() methode in FhirCheck, das heisst sobald
err() einmal aufgerufen wurde, ist ok nicht mehr ok.

``
        self.log.info(f"ended against {self.dbtarget}: "+
            str(aqtg_count) + " aliquot groups, " +
            str(master_count) + " master samples, " +
            str(derived_count) + " derived samples, " +
            str(len(entries)) + " total\n" )
        
        return self.ok # written by FhirCheck.err()
``

die importe fuer check_entries.

``/import
from dip import dig
from fhirproof.fhirhelp import fhirhelp as fh

from fhirproof.AqtMatCheck import *
from fhirproof.PrimaryInDbCheck import *
from fhirproof.DatesCheck import *
from fhirproof.LocationCheck import *
from fhirproof.BehealterCheck import *
from fhirproof.OUCheck import *
from fhirproof.ParentingCheck import *
from fhirproof.PsnCheck import *
from fhirproof.RestmengeCheck import *
from fhirproof.DerivmatCheck import *
from fhirproof.MayUserEditOUCheck import *
from fhirproof.IdContainerCheck import *
``

## check observations

check_observations checkt alle dateien in einem ein verzeichnis von observation files.

`/check_observations:
def check_observations(self, dir):

  # the collected entries
  entries = self.entries_from_dir(dir)

  self.check_observation_entries(entries)
`

check_observation_entries checkt die entries.

`/check_observation_entries:
  def check_observation_entries(self, entries):
    ``setup``
    ``loop``
    ``postloop``
    # todo continue
`

setze ok zurueck und initialisiere die checks. todo koennten die auch statisch sein?

`./setup:
    self.ok = True    
    blood_urine = ObsBloodUrine()

    self.log.info(f"starting observation check against {self.dbtarget}")
`

loope ueber die entries und lass die checks laufen.

`../loop:
    for entry in entries:
      blood_urine.check(entry)
`

gib ok zurueck, was von den checks beschrieben wurde.

`../postloop:
    return self.ok 
`

entries_from_dir returns the entries from all json files in a directory.

``/entries_from_dir:
def entries_from_dir(self, dir, encoding=None): # todo could be static?
  ``.``
``

if no encoding given assume utf-8.

``
  if encoding == None:
    encoding = "utf-8"
``

collect the entries from the files in the dir.

``
  entries = []
  
  files = os.listdir(dir)
  for file in files:
    ``.``
``

only consider json files.

``
    _, ext = os.path.splitext(file)
    # print("ext: " + ext)
    if ext != ".json":
      continue
``

load the json and append the entries it contains.

``
    with open(os.path.join(dir, file), "r", encoding=encoding) as f:    
      jsonin = json.load(f)

      for entry in dig(jsonin, "entry"):
        entries.append(entry)
``

return the entries.

``/entries_from_dir
  return entries
``

importiere os und json.

``/import
import os
import json
``

parent gibt die parent-resource eines entries zurueck, wenn es eine gibt.

auf parent() koennen die checks zugreifen.

``/parent:

def parent(self, entry):
    parent = None
    resource = entry['resource']
    if fh.type(resource) != "DERIVED":              
        return None
    # get fhirid of aliquotgroup-parent
    pfhirid = fh.parent_fhirid(resource)
    if pfhirid == None:
        return None
    elif pfhirid not in self.entrybyfhirid:
        return None
    return self.entrybyfhirid[pfhirid]['resource']
``

setuplog setzt den log auf.

``/setuplog:
    def _setuplog(self, logfile):
        # setup a logger to write to a file into logs folder
        log = logging.getLogger(__name__)
        log.setLevel(logging.INFO)
        file_handler = logging.FileHandler(logfile)
        formatter = logging.Formatter('%(asctime)s:%(levelname)s: %(message)s')
        file_handler.setFormatter(formatter)
        log.addHandler(file_handler)
        # log to stdout
        stdout_handler = logging.StreamHandler(sys.stdout)
        stdout_handler.setFormatter(formatter)
        log.addHandler(stdout_handler)
        self.log = log
``

importiere dazu logging und sys.

``/import
import logging
import sys
``
